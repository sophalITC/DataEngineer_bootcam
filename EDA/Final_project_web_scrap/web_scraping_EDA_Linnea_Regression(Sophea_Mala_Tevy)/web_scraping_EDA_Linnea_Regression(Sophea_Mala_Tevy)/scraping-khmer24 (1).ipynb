{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ignore warning message\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# to make request website\n",
    "import requests\n",
    "# to make html to soup \n",
    "from bs4 import BeautifulSoup\n",
    "# to use open source data analysis and manipulation\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base url website to scrape data\n",
    "base_url = 'https://www.khmer24.com/en'\n",
    "# category to scrape\n",
    "category = 'cars'\n",
    "# page param for number of records\n",
    "page_number = '?per_page='\n",
    "# the records/row per page\n",
    "rpp = 50\n",
    "dataset = 'data_scraped'\n",
    "category_directory = str.format('{}/{}', dataset, category)\n",
    "category_directory_url = str.format('{}/urls', category_directory, category)\n",
    "category_directory_url_file = str.format('{}/all_{}_url.csv', category_directory_url, category)\n",
    "category_directory_data = str.format('{}/datasets', category_directory, category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to request with user-agent and parse to html\n",
    "def get_request(url):\n",
    "    header = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(url, headers = header)\n",
    "    return BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get all urls from category and its type \n",
    "def get_category_urls(url = base_url, category = category):\n",
    "    data_return = []\n",
    "    page_url = str.format('{}/{}', url, category)\n",
    "    soup = get_request(page_url)\n",
    "    items = soup.findChildren('li', attrs = [{'class', ''}, {'class', 'd-none'}])\n",
    "    for i in items:\n",
    "        categories = {}\n",
    "        categories['url'] = i.a['href']\n",
    "        categories['model'] = i.a['title'].lower()\n",
    "        data_return.append(categories)\n",
    "    return data_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get all url from each type\n",
    "def get_all_url(url, model):\n",
    "    data_return = []\n",
    "    page = 0\n",
    "    while(True):\n",
    "        page_url = str.format('{}/{}{}', url, page_number, page)\n",
    "        soup = get_request(page_url)\n",
    "        items = soup.findChildren('a', attrs = {'class', 'border post'})\n",
    "        if(len(items) == 0):\n",
    "            break\n",
    "        for item in items:\n",
    "            data = {}\n",
    "            data['url'] = item['href']\n",
    "            data['model'] = model\n",
    "            data_return.append(data)\n",
    "        page = page + rpp\n",
    "    return data_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(my_str):\n",
    "    return my_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the detail of each url record\n",
    "def get_data_detail(url):\n",
    "    data = {}\n",
    "    soup = get_request(url)\n",
    "    # to get id, category, localtion and post date\n",
    "    posting_infos = soup.findChildren('ul', attrs = {'class', 'list-unstyled item-info m-0'})\n",
    "    for i in posting_infos:\n",
    "        for j in i.findChildren('li'):\n",
    "            data[remove_special_char(j.findChildren('span')[0].text)] = j.findChildren('span')[1].text\n",
    "    # to get description model, car makes, year, tax type, .... \n",
    "    descriptions = soup.findChildren('ul', attrs = {'class', 'list-unstyled item-fields'})\n",
    "    for i in descriptions:\n",
    "        for j in i.findChildren('li'):\n",
    "            data[remove_special_char(j.findChildren('span')[0].text)] = j.findChildren('span')[1].text\n",
    "    price = soup.findChild('p', attrs = {'class', 'price price_tag'})\n",
    "    if price is not None:\n",
    "        data['price'] = price.text\n",
    "    phone_numbers = soup.findChildren('ul', attrs = {'class', 'list-unstyled m-0'})\n",
    "    for i in phone_numbers:\n",
    "        for j in i.findChildren('li', attrs = {'class', 'number'}):\n",
    "            data[remove_special_char(j.find('a')['class'][0])] = j.a.find(attrs = {'class', 'num'}).text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_if_not_exist(file):\n",
    "    directory = os.path.dirname(file)\n",
    "    if directory is not '' and not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_dict(file_name):\n",
    "    data_return = None\n",
    "    try:\n",
    "        data_return = pd.read_csv(file_name).to_dict('records')\n",
    "    except Exception as er:\n",
    "        print(str.format('Read file error accured {}', er))\n",
    "    finally:\n",
    "        return data_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  to write data to csv file \n",
    "def write_data_to_csv(data, file_name):\n",
    "    file_name = remove_special_char(file_name)\n",
    "    create_directory_if_not_exist(file_name)\n",
    "    local_df = pd.DataFrame(data)\n",
    "    if file_name.endswith('.csv') == False:\n",
    "        file_name = str.format('{}.{}', file_name, 'csv')\n",
    "    local_df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detail_page(urls):\n",
    "    content_data = []\n",
    "    model = urls[0]['model']\n",
    "    file_name = str.format('{}/all_records_{}.csv', category_directory_data, model)\n",
    "    data = read_file_to_dict(file_name)\n",
    "    if data is None:\n",
    "        for j in urls:\n",
    "            item = get_data_detail(j['url'])\n",
    "            content_data.append(item)\n",
    "        write_data_to_csv(content_data, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_urls = read_file_to_dict(category_directory_url_file)\n",
    "if category_urls is None:\n",
    "    category_urls = get_category_urls()\n",
    "    write_data_to_csv(category_urls, category_directory_url_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2293 items for apple\n",
      "1018 items for samsung\n",
      "0281 items for huawei\n",
      "0044 items for sony\n",
      "0798 items for oppo\n",
      "0034 items for lg\n",
      "0367 items for vivo\n",
      "0086 items for nokia\n",
      "0006 items for meizu\n",
      "0056 items for oneplus\n",
      "0002 items for blackberry\n",
      "0002 items for htc\n",
      "0001 items for acer\n",
      "0098 items for google\n",
      "0316 items for xiaomi\n",
      "0029 items for motorola\n",
      "0001 items for alcatel\n",
      "0050 items for vertu\n",
      "0010 items for zte\n",
      "0037 items for asus\n",
      "0346 items for other - ផ្សេងៗ\n"
     ]
    }
   ],
   "source": [
    "for i in category_urls:\n",
    "    model = i['model']\n",
    "    file_name = str.format('{}/all_detail_url_{}.csv', category_directory_url, model)\n",
    "    data = read_file_to_dict(file_name)\n",
    "    if data is None:\n",
    "        data = get_all_url(i['url'], model)\n",
    "        write_data_to_csv(data, file_name)\n",
    "    else:\n",
    "        records = len(data)\n",
    "        print('{:0=4}'.format(records), 'items', 'for', model)\n",
    "        if records == 0:\n",
    "            # skip in case no records\n",
    "            continue\n",
    "    get_detail_page(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# byd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
